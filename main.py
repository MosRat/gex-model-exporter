import modeling_swin as org
from modeling_swin import SwinSelfAttention
import transformers.models
import transformers.models.swin.modeling_swin
from transformers.models.swin import modeling_swin

# ä½¿ç”¨patch çš„swinå®šä¹‰
modeling_swin.SwinSelfAttention = org.SwinSelfAttention
modeling_swin.SwinAttention = org.SwinAttention
modeling_swin.SwinModel = org.SwinModel
modeling_swin.SwinBackbone = org.SwinBackbone
modeling_swin.SwinStage = org.SwinStage
modeling_swin.SwinIntermediate = org.SwinIntermediate

from transformers import (
    VisionEncoderDecoderModel,
    AutoTokenizer,
    AutoImageProcessor,
    AutoModelForCausalLM,
)

import subprocess
import torch
import onnx
import onnxruntime
import numpy as np
from onnxsim import simplify
import torch.onnx
import warnings
from pathlib import Path
import argparse
import sys

warnings.filterwarnings("ignore")


def run_command(cmd, debug=False, message="", success_message=""):
    """
    è¿è¡Œ shell å‘½ä»¤å¹¶æ‰“å°è¿›åº¦ã€‚
    å‚æ•°:
        cmd (list): å‘½ä»¤åŠå…¶å‚æ•°çš„åˆ—è¡¨ã€‚
        debug (bool): å¦‚æœä¸º Trueï¼Œåˆ™æ‰“å°å­è¿›ç¨‹çš„æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯ã€‚
        message (str): è¿è¡Œå‘½ä»¤å‰è¦æ‰“å°çš„æ¶ˆæ¯ã€‚
        success_message (str): æˆåŠŸå®Œæˆæ—¶è¦æ‰“å°çš„æ¶ˆæ¯ã€‚
    è¿”å›:
        bool: å¦‚æœå‘½ä»¤æˆåŠŸåˆ™ä¸º Trueï¼Œå¦åˆ™ä¸º Falseã€‚
    """
    if message:
        print(f"âœ¨ {message}...", end="", flush=True)

    # æ ¹æ® debug æ¨¡å¼æ•è·æˆ–æ˜¾ç¤ºè¾“å‡º
    result = subprocess.run(cmd, capture_output=not debug, text=True)

    if result.returncode == 0:
        if message:
            print("âœ…")
        if success_message:
            print(f"âœ… {success_message}")
        return True
    else:
        if message:
            print("âŒ")
        print(f"é”™è¯¯ï¼šå‘½ä»¤æ‰§è¡Œå¤±è´¥: {' '.join(cmd)}\n è¿”å› {result.returncode}")
        if not debug:
            if result.stdout:
                print("STDOUT:\n", result.stdout)
            if result.stderr:
                print("STDERR:\n", result.stderr)
        return False


def fix_attn_op(model_path, debug=False):
    """
    ä¿®å¤ ONNX æ¨¡å‹ä¸­çš„ Attention æ“ä½œå¹¶è¿›è¡Œç®€åŒ–ã€‚
    """
    print(f"âœ¨ ä¿®å¤å¹¶ç®€åŒ– ONNX æ¨¡å‹: {model_path.name}...", end="", flush=True)
    try:
        model_onnx = onnx.load(model_path)

        # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        for node in model_onnx.graph.node:
            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å±äºcom.microsoftåŸŸ
            if node.domain == "com.microsoft" and "Attention" in node.op_type:
                if debug:
                    print(f"\n  - å‘ç° Attention èŠ‚ç‚¹: {node.name}", end="", flush=True)
                # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰4ä¸ªè¾“å…¥
                if len(node.input) == 4:
                    # åœ¨ç¬¬3å’Œç¬¬4ä¸ªè¾“å…¥ä¹‹é—´æ’å…¥ä¸¤ä¸ªç©ºè¾“å…¥
                    node.input.insert(3, "")
                    node.input.insert(4, "")
                    if debug:
                        print(" (å·²æ’å…¥ç©ºè¾“å…¥)", end="", flush=True)

        # å¯¹äº ONNX ç®€åŒ–ï¼Œå‡è®¾è¾“å…¥å½¢çŠ¶ä¸º 1, 3, 448, 448
        initial_input_shape = {"pixel_values": (1, 3, 448, 448)}

        model_onnx, check = simplify(
            model_onnx, overwrite_input_shapes=initial_input_shape
        )
        if not check and debug:
            print("â— ONNX ç®€åŒ–æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†ã€‚", end="", flush=True)

        onnx.save(model_onnx, model_path)
        print("âœ…")
        return True
    except Exception as e:
        print("âŒ")
        print(f"ONNX ä¿®å¤å’Œç®€åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def export_mixtex_encoder(hf_model, output_dir, debug=False):
    """
    å°† MixTex ç¼–ç å™¨å¯¼å‡ºä¸º ONNX æ ¼å¼ã€‚
    """
    model_path = output_dir / "encoder.onnx"
    model = hf_model

    # ç¡®ä¿ç¼–ç å™¨çš„ pooler ä¸º Noneï¼Œä»¥ä¾¿ ONNX å¯¼å‡º
    if hasattr(model.encoder, "pooler"):
        model.encoder.pooler = None

    fake_input = torch.randn(1, 3, 448, 448, dtype=torch.float32, device=model.device)

    # è½¬æ¢ SwinSelfAttention ä¸­çš„ QKV
    for m in model.encoder.modules():
        if isinstance(m, SwinSelfAttention):
            m.convert_qkv()

    # å†æ¬¡è®¾ç½® pooler ä¸º Noneï¼Œä»¥é˜²å®ƒè¢«é‡æ–°èµ‹å€¼
    if hasattr(model.encoder, "pooler"):
        model.encoder.pooler = None

    print("âœ¨ è½¬æ¢ç¼–ç å™¨ä¸º ONNX æ ¼å¼...", end="", flush=True)
    try:
        with torch.inference_mode():
            _ = torch.onnx.export(
                model.encoder,
                (fake_input,),
                model_path,
                input_names=["pixel_values"],
                output_names=["last_hidden_state"],
                opset_version=20,
                # å¦‚æœéœ€è¦åŠ¨æ€æ‰¹å¤„ç†å¤§å°ï¼Œå¯ä»¥æ·»åŠ  dynamic_axes={"pixel_values": {0: "batch_size"}, "last_hidden_state": {0: "batch_size"}}
            )
        print("âœ…")
    except Exception as e:
        print("âŒ")
        print(f"ONNX å¯¼å‡ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False

    if not fix_attn_op(model_path, debug):
        return False

    print("âœ¨ éªŒè¯ ONNX å¯¼å‡ºç»“æœ...", end="", flush=True)
    try:
        o = model.encoder(fake_input)[0].detach().cpu().numpy()

        s = onnxruntime.InferenceSession(str(model_path))  # å°† Path å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        o_s = s.run(
            None,
            {
                "pixel_values": fake_input.detach().cpu().numpy(),
            },
        )[0]

        assert np.allclose(o, o_s, rtol=1e-4, atol=1e-4)
        print("âœ…")
        if debug:
            print(f"PyTorch å’Œ ONNX è¾“å‡ºçš„å¹³å‡ç»å¯¹å·®: {np.mean(np.abs(o - o_s))}")
        return True
    except Exception as e:
        print("âŒ")
        print(f"ONNX éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def export_mixtex_decoder(
    model_cache_dir, output_dir, quant_type="Q4_K_M", debug=False
):
    """
    å°† MixTex è§£ç å™¨å¯¼å‡ºå¹¶é‡åŒ–ä¸º GGUF æ ¼å¼ã€‚
    """
    decoder_fp32_gguf = output_dir / "mixtex-dec_fp32.gguf"
    decoder_quant_gguf = output_dir / f"mixtex-dec-{quant_type}.gguf"

    # è½¬æ¢ HF æ¨¡å‹ä¸º GGUF (fp32)
    cmd = [
        "uv",
        "run",
        "convert_hf_to_gguf.py",
        f"{str(model_cache_dir)}",
        "--outfile",
        str(decoder_fp32_gguf),
    ]
    if not run_command(
        cmd, debug, f"è½¬æ¢è§£ç å™¨ä¸º fp32 GGUF ({decoder_fp32_gguf.name})"
    ):
        return False

    # é‡åŒ– GGUF
    quantizer_bin = (
        Path("bin/llama-quantize.exe")
        if sys.platform == "win32"
        else Path("bin/llama-quantize")
    )
    # ç¡®ä¿ llama-quantize å·¥å…·å­˜åœ¨äºé¢„æœŸçš„è·¯å¾„
    if not quantizer_bin.exists():
        print(f"é”™è¯¯ï¼šé‡åŒ–å™¨äºŒè¿›åˆ¶æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {quantizer_bin.absolute()}")
        return False

    cmd = [
        str(quantizer_bin.absolute()),
        str(decoder_fp32_gguf),
        str(decoder_quant_gguf),
        quant_type,
    ]
    if not run_command(
        cmd, debug, f"é‡åŒ–è§£ç å™¨ä¸º {quant_type} GGUF ({decoder_quant_gguf.name})"
    ):
        return False

    return True


def export_gex(model_cache_dir, output_dir, quant_type="Q4_K_M", debug=False):
    """
    å°† GexT æ¨¡å‹ (è§£ç å™¨å’Œ mmproj) å¯¼å‡ºå¹¶é‡åŒ–ä¸º GGUF æ ¼å¼ã€‚
    """
    gext_fp32_gguf = output_dir / "gext_fp32.gguf"
    gext_quant_gguf = output_dir / f"gext-{quant_type}.gguf"
    mmproj_fp32_gguf = output_dir / "mmproj_fp32.gguf"

    # è½¬æ¢ HF æ¨¡å‹ä¸º GGUF (è§£ç å™¨ fp32)
    cmd_gext = [
        "uv",
        "run",
        "convert_hf_to_gguf.py",
        f"{str(model_cache_dir)}",
        "--outfile",
        str(gext_fp32_gguf),
    ]
    if not run_command(
        cmd_gext, debug, f"è½¬æ¢ GexT è§£ç å™¨ä¸º fp32 GGUF ({gext_fp32_gguf.name})"
    ):
        return False

    # è½¬æ¢ HF æ¨¡å‹ä¸º GGUF (mmproj fp32)
    cmd_mmproj = [
        "uv",
        "run",
        "convert_hf_to_gguf.py",
        f"{str(model_cache_dir)}",
        "--outfile",
        str(mmproj_fp32_gguf),
        "--mmproj",
    ]
    if not run_command(
        cmd_mmproj, debug, f"è½¬æ¢ GexT mmproj ä¸º fp32 GGUF ({mmproj_fp32_gguf.name})"
    ):
        return False

    # é‡åŒ– GexT è§£ç å™¨
    quantizer_bin = (
        Path("bin/llama-quantize.exe")
        if sys.platform == "win32"
        else Path("bin/llama-quantize")
    )
    if not quantizer_bin.exists():
        print(f"é”™è¯¯ï¼šé‡åŒ–å™¨äºŒè¿›åˆ¶æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {quantizer_bin.absolute()}")
        return False

    cmd_quant = [
        str(quantizer_bin.absolute()),
        str(gext_fp32_gguf),
        str(gext_quant_gguf),
        quant_type,
    ]
    if not run_command(
        cmd_quant,
        debug,
        f"é‡åŒ– GexT è§£ç å™¨ä¸º {quant_type} GGUF ({gext_quant_gguf.name})",
    ):
        return False

    # æ³¨æ„: åŸå§‹è„šæœ¬æ²¡æœ‰é‡åŒ– mmprojï¼Œåªè½¬æ¢ä¸º fp32ã€‚å¦‚æœéœ€è¦é‡åŒ– mmprojï¼Œè¯·åœ¨æ­¤å¤„æ·»åŠ é¢å¤–çš„é‡åŒ–æ­¥éª¤ã€‚

    return True


def main():
    parser = argparse.ArgumentParser(
        description="å°† Hugging Face æ¨¡å‹è½¬æ¢ä¸º ONNX å’Œ GGUF æ ¼å¼ã€‚"
    )
    parser.add_argument(
        "--hf_model_path",
        type=str,
        required=True,
        help="Hugging Face æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: 'MosRat/GexT_V1' æˆ– 'MixTex/base_ZhEn')ã€‚",
    )
    parser.add_argument(
        "--model_type",
        type=str,
        choices=["mixtex", "gext"],
        required=True,
        help="æ¨¡å‹ç±»å‹: 'mixtex' (ç¼–ç å™¨-è§£ç å™¨) æˆ– 'gext' (AutoModelForCausalLM)ã€‚",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./converted_models",
        help="ä¿å­˜è½¬æ¢åçš„ ONNX å’Œ GGUF æ¨¡å‹çš„ç›®å½•ã€‚",
    )
    parser.add_argument(
        "--quant_type",
        type=str,
        default="Q4_K_M",
        help="GGUF æ¨¡å‹çš„é‡åŒ–ç±»å‹ (ä¾‹å¦‚: Q4_K_M, Q5_K_M)ã€‚",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼ä»¥æ˜¾ç¤ºå®Œæ•´çš„å­è¿›ç¨‹è¾“å‡ºå’Œæ›´è¯¦ç»†çš„æ¶ˆæ¯ã€‚",
    )

    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨

    print(
        f"\nğŸš€ å¼€å§‹æ¨¡å‹è½¬æ¢: {args.hf_model_path} (ç±»å‹: {args.model_type.upper()})..."
    )
    print(f"ğŸ“¦ è¾“å‡ºå°†ä¿å­˜åˆ°: {output_dir.absolute()}")
    print(f"âš™ï¸ é‡åŒ–ç±»å‹: {args.quant_type}")
    if args.debug:
        print("ğŸ è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ã€‚")
    print("-" * 50)

    # --- åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    # ä¸ºæ¨¡å‹åœ¨æœ¬åœ°çš„ç¼“å­˜åˆ›å»ºä¸€ä¸ªä¸“ç”¨ç›®å½•
    model_name_for_dir = args.hf_model_path.replace("/", "_")
    model_cache_dir = Path("./hf_models") / model_name_for_dir
    model_cache_dir.mkdir(exist_ok=True, parents=True)  # ç¡®ä¿åŸºæœ¬ç¼“å­˜ç›®å½•å­˜åœ¨

    tokenizer = None
    hf_model = None

    try:
        print("âœ¨ åŠ è½½åˆ†è¯å™¨...", end="", flush=True)
        # å°† cache_dir è®¾ç½®ä¸ºæ¨¡å‹ä¸“å±çš„æœ¬åœ°ç¼“å­˜ç›®å½•
        tokenizer = AutoTokenizer.from_pretrained(
            args.hf_model_path, trust_remote_code=True, cache_dir=model_cache_dir
        )
        print("âœ…")

        print("âœ¨ åŠ è½½ Hugging Face æ¨¡å‹...", end="", flush=True)
        if args.model_type == "mixtex":
            hf_model = VisionEncoderDecoderModel.from_pretrained(
                args.hf_model_path, cache_dir=model_cache_dir
            ).eval()
        elif args.model_type == "gext":
            hf_model = AutoModelForCausalLM.from_pretrained(
                args.hf_model_path, trust_remote_code=True, cache_dir=model_cache_dir
            ).eval()
        print("âœ…")

        # å°†æ¨¡å‹å’Œåˆ†è¯å™¨ä¿å­˜åˆ°ç¼“å­˜ç›®å½•ï¼Œä»¥ä¾¿ `convert_hf_to_gguf.py` å¯ä»¥è®¿é—®å®ƒä»¬
        print(f"âœ¨ ç¼“å­˜æ¨¡å‹å’Œåˆ†è¯å™¨åˆ° {model_cache_dir}...", end="", flush=True)
        hf_model.save_pretrained(model_cache_dir)
        tokenizer.save_pretrained(model_cache_dir)
        # å¯¹äº MixTexï¼Œå¦‚æœå­˜åœ¨ï¼Œä¹Ÿä¿å­˜ feature_extractor
        if args.model_type == "mixtex":
            try:
                feature_extractor = AutoImageProcessor.from_pretrained(
                    args.hf_model_path, cache_dir=model_cache_dir
                )
                feature_extractor.save_pretrained(model_cache_dir)
            except Exception as fe_e:
                if args.debug:
                    print(
                        f"\nè­¦å‘Š: æ— æ³•åŠ è½½/ä¿å­˜ feature extractor (å¯èƒ½ä¸å­˜åœ¨): {fe_e}"
                    )
        print("âœ…")

    except Exception as e:
        print("âŒ")
        print(f"ä» Hugging Face åŠ è½½æ¨¡å‹æˆ–åˆ†è¯å™¨å¤±è´¥: {e}")
        sys.exit(1)  # åŠ è½½å¤±è´¥åˆ™é€€å‡º

    # --- æ ¹æ®æ¨¡å‹ç±»å‹æ‰§è¡Œè½¬æ¢ ---
    success = False
    if args.model_type == "mixtex":
        print("\n--- å¯¼å‡º MixTex ç¼–ç å™¨ (ONNX) ---")
        if export_mixtex_encoder(hf_model, output_dir, args.debug):
            print("\n--- å¯¼å‡º MixTex è§£ç å™¨ (GGUF) ---")
            success = export_mixtex_decoder(
                model_cache_dir, output_dir, args.quant_type, args.debug
            )
    elif args.model_type == "gext":
        # GexT æ¨¡å‹ç›´æ¥é€šè¿‡ convert_hf_to_gguf.py å¤„ç†ï¼ŒåŒ…æ‹¬å…¶è§†è§‰éƒ¨åˆ† (mmproj)
        print("\n--- å¯¼å‡º GexT (GGUF) ---")
        success = export_gex(model_cache_dir, output_dir, args.quant_type, args.debug)

    print("-" * 50)
    if success:
        print("ğŸ‰ è½¬æ¢è¿‡ç¨‹å®ŒæˆæˆåŠŸï¼")
    else:
        print("ğŸ’” è½¬æ¢è¿‡ç¨‹å¤±è´¥ã€‚")
        sys.exit(1)  # è½¬æ¢å¤±è´¥åˆ™é€€å‡º


if __name__ == "__main__":
    main()
